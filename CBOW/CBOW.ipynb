{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "from math import ceil\n",
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pylab\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"processed_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'r') as f:\n",
    "    words = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING DATA........\n",
      "Data size 6868221\n",
      "First 10 words:  ['description', 'of', 'farmer', 'oak', 'an', 'incident', 'when', 'farmer', 'oak', 'smiled']\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_file):\n",
    "    data = []\n",
    "    print(\"READING DATA........\")\n",
    "    tokenized = nltk.word_tokenize(data_file)\n",
    "    data.extend(tokenized)\n",
    "    return data\n",
    "\n",
    "words = read_data(words)\n",
    "print('Data size %d' % len(words))\n",
    "print('First 10 words: ', words[:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing data in different manners\n",
    "- dictionary: maps a word to an ID\n",
    "- reverse_dictionary: maps an ID to a word\n",
    "- count: stores the frequency of a word in a list of tuples\n",
    "- data: stores the words as their IDs\n",
    " \n",
    "This will help us in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [0, 4, 162, 52, 42, 2138, 49, 162, 52, 1481]\n",
      "Most common words:  [['UNK', 113980], ('the', 391991), ('and', 216126), ('a', 196601), ('of', 191339), ('to', 178043), ('in', 118910), ('i', 101234), ('was', 99654), ('it', 78347)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10000\n",
    "\n",
    "def create_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    # we only choose the most frequently used words as our vocab rest will be replaced with UNK \n",
    "    # 'collections' will give us the most frequent words for a certain length\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    \n",
    "    # mapping word to ID\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) # this will give IDs to all words as with each iteration 'len' increases\n",
    "        \n",
    "    # stores words as theirs IDs\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        # If a word in dictionay then we use ID else it will be 'UNK'\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0 # id of UNK\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    \n",
    "    # total UNK in dataset\n",
    "    count[0][1] = unk_count \n",
    "\n",
    "    # mapping ID to word\n",
    "    # here we interchanged the positions, from (keys:values) to (values:keys)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    \n",
    "    # making sure the dict is of the size of vocab\n",
    "    assert len(dictionary) == vocabulary_size # this is like a checker if true then pass else raise AssertionError\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "    \n",
    "data, count, dictionary, reverse_dictionary = create_dataset(words)\n",
    "print('Data: ', data[:10])\n",
    "print('Most common words: ' ,count[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batches of data for CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with window_size = 1:\n",
      "    batch: [['UNK', 'farmer'], ['of', 'oak'], ['farmer', 'an'], ['oak', 'incident'], ['an', 'when'], ['incident', 'farmer'], ['when', 'oak'], ['farmer', 'smiled']]\n",
      "    labels: ['of', 'farmer', 'oak', 'an', 'incident', 'when', 'farmer', 'oak']\n",
      "\n",
      "with window_size = 2:\n",
      "    batch: [['UNK', 'of', 'oak', 'an'], ['of', 'farmer', 'an', 'incident'], ['farmer', 'oak', 'incident', 'when'], ['oak', 'an', 'when', 'farmer'], ['an', 'incident', 'farmer', 'oak'], ['incident', 'when', 'oak', 'smiled'], ['when', 'farmer', 'smiled', 'the'], ['farmer', 'oak', 'the', 'corners']]\n",
      "    labels: ['farmer', 'oak', 'an', 'incident', 'when', 'farmer', 'oak', 'smiled']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def create_cbow_batch(batch_size, window_size):\n",
    "    # We will update the data_index everytime we read a set of data point\n",
    "    global data_index\n",
    "    \n",
    "    span = 2 * window_size + 1 # [skip_window target skip_window]\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size, span -1), dtype = np.int32)\n",
    "    labels= np.ndarray(shape=(batch_size, 1), dtype = np.int32)\n",
    "\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size):\n",
    "        target = window_size  \n",
    "        target_to_avoid = [ window_size ] \n",
    "        col_idx = 0\n",
    "        for j in range(span):\n",
    "            if j==span//2:\n",
    "                continue\n",
    "            batch[i,col_idx] = buffer[j] \n",
    "            col_idx += 1\n",
    "        labels[i, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "for window_size in [1,2]:\n",
    "    data_index = 0\n",
    "    batch, labels = create_cbow_batch(batch_size=8, window_size=window_size)\n",
    "    print('\\nwith window_size = %d:' % (window_size))\n",
    "    print('    batch:', [[reverse_dictionary[bii] for bii in bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "window_size = 2\n",
    "\n",
    "#validation\n",
    "valid_size = 16\n",
    "valid_window = 50\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n",
    "\n",
    "num_sampled = 32 # Number of negative examples to sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_dataset = tf.placeholder(tf.int32, shape=[batch_size, 2*window_size])\n",
    "train_labels= tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "# Embedding layer, contains the word embeddings\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0,dtype=tf.float32))\n",
    "\n",
    "# Softmax Weights and Biases\n",
    "softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                 stddev=0.5 / math.sqrt(embedding_size),dtype=tf.float32))\n",
    "softmax_biases = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining 4 embedding lookups representing each word in the context\n",
      "Stacked embedding size: [128, 128, 4]\n",
      "Reduced mean embedding size: [128, 128]\n"
     ]
    }
   ],
   "source": [
    "# Model.\n",
    "# Look up embeddings for a batch of inputs.\n",
    "# Here we do embedding lookups for each column in the input placeholder\n",
    "# and then average them to produce an embedding_size word vector\n",
    "stacked_embedings = None\n",
    "print('Defining %d embedding lookups representing each word in the context'%(2*window_size))\n",
    "for i in range(2*window_size):\n",
    "    embedding_i = tf.nn.embedding_lookup(embeddings, train_dataset[:,i])        \n",
    "    x_size,y_size = embedding_i.get_shape().as_list()\n",
    "    if stacked_embedings is None:\n",
    "        stacked_embedings = tf.reshape(embedding_i,[x_size,y_size,1])\n",
    "    else:\n",
    "        stacked_embedings = tf.concat(axis=2,values=[stacked_embedings,tf.reshape(embedding_i,[x_size,y_size,1])])\n",
    "\n",
    "assert stacked_embedings.get_shape().as_list()[2]==2*window_size\n",
    "print(\"Stacked embedding size: %s\"%stacked_embedings.get_shape().as_list())\n",
    "mean_embeddings =  tf.reduce_mean(stacked_embedings,2,keepdims=False)\n",
    "print(\"Reduced mean embedding size: %s\"%mean_embeddings.get_shape().as_list())\n",
    "\n",
    "# Compute the softmax loss, using a sample of the negative labels each time.\n",
    "# inputs are embeddings of the train words\n",
    "# with this loss we optimize weights, biases, embeddings\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=mean_embeddings,\n",
    "                           labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer \n",
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting word similarities from cosine distance\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 2000: 0.888345\n",
      "Average loss at step 4000: 0.814543\n",
      "Average loss at step 6000: 0.783365\n",
      "Average loss at step 8000: 0.786896\n",
      "Average loss at step 10000: 0.789523\n",
      "Nearest to were: buttons, joe, corrected, informant, marketday, bess, compounds, attending,\n",
      "Nearest to for: culminated, blooming, creaks, density, narrowness, generalities, wandering, tides,\n",
      "Nearest to one: convulsions, repliedi, paths, privilege, warm, olbscured, shame, twilight,\n",
      "Nearest to was: changed, instincts, trained, murmuring, purse, villains, threatening, screen,\n",
      "Nearest to and: pulse, profits, guessing, rarest, snakes, ho, resentment, francis,\n",
      "Nearest to his: wilinot, plainly, neck, tripped, strangulation, picking, prepara, bliss,\n",
      "Nearest to as: stalactite, wool, lanterns, affairs, weighted, delicate, hireling, holding,\n",
      "Nearest to be: touched, have, laughing, tomb, leastwise, endeavours, corresponding, unassumingly,\n",
      "Nearest to so: beg, conver, icicles, habit, dun, negative, impeachable, languor,\n",
      "Nearest to at: wellknown, shivered, hope, contribute, area, suspend, minddwelt, shrubs,\n",
      "Nearest to in: mination, protected, weighs, grassplot, somebody, prompt, torturing, maam,\n",
      "Nearest to now: elbows, pouring, freedom, labour, lies, hnowing, garded, allowed,\n",
      "Nearest to the: crossing, pressions, talking, frenzies, roasting, absorbed, exasperating, whereupon,\n",
      "Nearest to said: mellowed, actually, landing, soldier, stili, chanced, worthier, saluted,\n",
      "Nearest to have: be, orbits, parted, strictly, voking, hearth, vacantly, washing,\n",
      "Nearest to an: blank, stalls, likewise, g, prent, tirade, flattery, stained,\n",
      "Average loss at step 12000: 0.809486\n",
      "Average loss at step 14000: 0.796043\n",
      "Average loss at step 16000: 0.792119\n",
      "Average loss at step 18000: 0.788032\n",
      "Average loss at step 20000: 0.803315\n",
      "Nearest to were: bitterness, joe, buttons, springing, attending, notathomes, corrected, askance,\n",
      "Nearest to for: miller, gloomier, sluices, slily, delighted, varnish, culminated, buy,\n",
      "Nearest to one: repliedi, paths, snowy, convulsions, bass, originating, olbscured, horned,\n",
      "Nearest to was: changed, clump, murmuring, screen, trained, instincts, is, compliance,\n",
      "Nearest to and: kerb, tastes, forwardness, guessing, wing, tantalizing, townbred, motionless,\n",
      "Nearest to his: plainly, strangulation, lights, kinds, pairs, pharaoh, aboat, sensations,\n",
      "Nearest to as: affairs, quenched, darling, stalactite, plum, access, hireling, bedclothes,\n",
      "Nearest to be: fun, tively, correct, have, corresponding, vandyked, shaping, rural,\n",
      "Nearest to so: watches, beg, impeachable, upward, dun, conver, although, attentively,\n",
      "Nearest to at: vehicle, confined, minddwelt, mere, wretchedness, relinquished, alight, superincumbent,\n",
      "Nearest to in: mination, called, wake, revenge, m, valleys, mused, notied,\n",
      "Nearest to now: haired, labour, jealousy, drawn, pouring, youre, nape, lies,\n",
      "Nearest to the: those, northwestbynorth, foreseen, retiring, unbroken, creatures, st, oblique,\n",
      "Nearest to said: putt, columbus, chanced, continued, unlike, fired, plunged, rattled,\n",
      "Nearest to have: encourage, bristled, temperate, vacantly, be, georges, feel, parted,\n",
      "Nearest to an: likewise, blank, experiences, prent, circling, ceremony, lanternlight, move,\n",
      "Average loss at step 22000: 0.810522\n",
      "Average loss at step 24000: 0.803858\n",
      "Average loss at step 26000: 0.797772\n",
      "Average loss at step 28000: 0.800105\n",
      "Average loss at step 30000: 0.821538\n",
      "Nearest to were: springing, are, notathomes, joe, ness, bitterness, ostensible, fiftyseven,\n",
      "Nearest to for: miller, sluices, culminated, slily, saw, beast, handsome, conclusive,\n",
      "Nearest to one: repliedi, olbscured, snowy, course, drawlatching, turnpikeman, dotted, pucker,\n",
      "Nearest to was: instincts, is, murmuring, awaited, lung, sole, impressions, starlight,\n",
      "Nearest to and: inconsequent, unconcerned, horses, capital, importance, emphatically, grave, wing,\n",
      "Nearest to his: strangulation, plainly, your, eleventh, animated, caverns, damned, skeins,\n",
      "Nearest to as: hay, demonian, weighted, tastes, comrades, stalactite, trial, supplants,\n",
      "Nearest to be: have, import, hey, freeing, moth, correct, blessing, rural,\n",
      "Nearest to so: spirit, aesthetic, dun, encased, dragoon, upward, outhouse, rude,\n",
      "Nearest to at: area, shivered, kicked, suspend, detached, divination, froward, shrubs,\n",
      "Nearest to in: flapped, signifying, york, wake, song, revenge, hey, meditated,\n",
      "Nearest to now: magnanimously, air, pouring, hammered, faithful, enterprise, coin, youre,\n",
      "Nearest to the: coveted, weatherbury, sandiness, oblique, promontor, unneces, fiddlers, moss,\n",
      "Nearest to said: continued, guides, grave, tossed, nicety, rattled, moorish, instanly,\n",
      "Nearest to have: encourage, temperate, vacantly, be, strictly, corpse, are, had,\n",
      "Nearest to an: lanternlight, accursed, blank, ceremony, beeches, flattery, circling, g,\n",
      "Average loss at step 32000: 0.817382\n",
      "Average loss at step 34000: 0.854687\n",
      "Average loss at step 36000: 0.854816\n",
      "Average loss at step 38000: 0.854915\n",
      "Average loss at step 40000: 0.866402\n",
      "Nearest to were: joe, upwards, ness, floated, bitterness, are, ostensible, springing,\n",
      "Nearest to for: unplaned, incompre, centres, culminated, sluices, conclusive, miller, occa,\n",
      "Nearest to one: unexplored, nowadays, drawlatching, milestone, grocers, profane, certificate, dotted,\n",
      "Nearest to was: is, murmuring, awaited, spruce, impressions, assumption, am, contrived,\n",
      "Nearest to and: answering, mouthful, ranged, waggoners, across, pulls, comebychance, thoughtlessness,\n",
      "Nearest to his: dotted, strangulation, liddys, copperplate, fannys, plainly, her, kinds,\n",
      "Nearest to as: complainingly, stalactite, sovereigns, vacantly, newfound, stereotyped, quenched, consideration,\n",
      "Nearest to be: rural, heaved, have, blindness, sented, complements, ies, treetrunks,\n",
      "Nearest to so: encased, spirit, impeachable, scribbled, garish, songs, vitals, shep,\n",
      "Nearest to at: confined, area, kicked, goodmen, brandy, shrubs, gallantries, ramble,\n",
      "Nearest to in: york, flapped, wake, luncheon, hey, unbidden, minerva, defence,\n",
      "Nearest to now: tme, hnowing, futurity, ceeded, ascended, evidently, lies, escapade,\n",
      "Nearest to the: jove, foreshores, injustice, possess, silvery, chandeliers, griffins, trite,\n",
      "Nearest to said: continued, because, guides, thought, instanly, duet, chose, breezes,\n",
      "Nearest to have: encourage, think, be, cadence, bcll, exemplified, ginger, credulous,\n",
      "Nearest to an: blank, lanternlight, discord, beeches, a, dye, lairs, mood,\n",
      "Average loss at step 42000: 0.882101\n",
      "Average loss at step 44000: 0.892030\n",
      "Average loss at step 46000: 0.875024\n",
      "Average loss at step 48000: 0.881000\n",
      "Average loss at step 50000: 0.876058\n",
      "Nearest to were: are, dirt, oblivious, inquest, upwards, mastershearer, promises, drawback,\n",
      "Nearest to for: buy, bump, essence, overtaken, shearer, incompre, caused, firmly,\n",
      "Nearest to one: grocers, unheeding, addition, viewed, fallow, diffidently, party, unanimity,\n",
      "Nearest to was: murmuring, slavery, seemed, friendship, is, sowed, pricker, relatively,\n",
      "Nearest to and: townbred, garb, grimy, compact, ambassador, strove, sober, sped,\n",
      "Nearest to his: copperplate, merged, strangulation, kinds, liddys, her, my, pharaoh,\n",
      "Nearest to as: cork, vices, harnessediook, stalactite, withdrawing, largest, ann, weighted,\n",
      "Nearest to be: grateful, king, myself, complements, ency, decisively, board, protect,\n",
      "Nearest to so: encased, exhibiting, artistic, cowshed, rude, garish, signifting, dun,\n",
      "Nearest to at: embarrassment, promising, bashful, scarn, vicious, association, simple, plain,\n",
      "Nearest to in: horizontal, into, drily, overhung, assuredly, breast, amply, definition,\n",
      "Nearest to now: futurity, abnormal, requires, whist, meanings, jan, guide, ceeded,\n",
      "Nearest to the: promontor, personality, stank, astonishment, foliage, pride, bold, coveted,\n",
      "Nearest to said: couldnt, newborn, drab, continued, thought, inquired, say, believe,\n",
      "Nearest to have: graphical, had, lowering, thor, corroborated, straight, know, assure,\n",
      "Nearest to an: bearable, untie, perforce, beeches, winningly, blank, lairs, accursed,\n",
      "Average loss at step 52000: 0.846866\n",
      "Average loss at step 54000: 0.622273\n",
      "Average loss at step 56000: 0.613939\n",
      "Average loss at step 58000: 0.563256\n",
      "Average loss at step 60000: 0.582711\n",
      "Nearest to were: are, marketday, fooled, enemy, notathomes, abiding, piety, upwards,\n",
      "Nearest to for: sluices, darting, straight, o, puddens, ahsolutely, centres, towards,\n",
      "Nearest to one: repliedi, convulsions, addition, assuming, milestone, ebb, warm, fallow,\n",
      "Nearest to was: lane, am, instincts, murmuring, stripling, pricker, carrier, wasnt,\n",
      "Nearest to and: pore, eveybody, extension, number, travelled, mur, sarily, bachelorship,\n",
      "Nearest to his: tripped, strangulation, sensations, helpless, wilinot, pharaoh, mused, copperplate,\n",
      "Nearest to as: weighted, motions, thers, stalactite, quenched, onward, ensue, sober,\n",
      "Nearest to be: have, repeat, take, comeliness, imposed, steps, make, complements,\n",
      "Nearest to so: dun, garish, rude, regardful, dirt, impeachable, cured, must,\n",
      "Nearest to at: expostulated, crossed, promising, ascertain, bashful, ramble, forwardness, fortunate,\n",
      "Nearest to in: horizontal, noteworthy, shirtsleeve, breast, ranging, assuredly, amply, mailcart,\n",
      "Nearest to now: hnowing, requires, vowed, laughing, fleed, jealousy, discerning, labour,\n",
      "Nearest to the: pressions, dubiously, espalier, vexed, injustice, pelling, plumpness, tethering,\n",
      "Nearest to said: continued, sail, missing, exclaimed, pinch, columbus, rest, trick,\n",
      "Nearest to have: ewe, be, tide, transepts, previously, strictly, lowering, help,\n",
      "Nearest to an: blank, circling, beeches, removing, abruptly, carriers, enactment, ceremony,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 62000: 0.577793\n",
      "Average loss at step 64000: 0.575684\n",
      "Average loss at step 66000: 0.559593\n",
      "Average loss at step 68000: 0.570851\n",
      "Average loss at step 70000: 0.548289\n",
      "Nearest to were: are, upwards, enemy, fooled, ness, impressions, marketday, elderly,\n",
      "Nearest to for: sluices, overtaken, spring, augured, fitness, culminated, hensible, eyeletholes,\n",
      "Nearest to one: addition, repliedi, tophet, easier, fine, another, milestone, stalls,\n",
      "Nearest to was: is, phases, wasnt, freshening, sowed, carrier, trained, lodged,\n",
      "Nearest to and: guessing, shook, pore, incurable, purification, profits, bachelorship, lance,\n",
      "Nearest to his: your, strangulation, tripped, ascent, sensations, liddys, plainly, hats,\n",
      "Nearest to as: quenched, weighted, stalactite, doesnt, onward, terminated, affairs, curve,\n",
      "Nearest to be: have, nonfulfilment, complements, use, tracks, sexajessamine, mystified, moth,\n",
      "Nearest to so: dun, bond, gallantries, mainly, groom, garish, amble, rude,\n",
      "Nearest to at: minddwelt, promising, stack, hark, expostulated, gage, crossed, hiccupped,\n",
      "Nearest to in: noteworthy, throws, overthrown, mystifier, distended, eagle, pitchhalfpenny, wake,\n",
      "Nearest to now: hnowing, recruits, jan, vowed, laughing, excitedly, futurity, haired,\n",
      "Nearest to the: troys, partlyopened, aternoon, fortyone, ex, crabbed, pressions, rows,\n",
      "Nearest to said: continued, resolved, looked, instanly, columbus, murmured, vital, inquired,\n",
      "Nearest to have: be, admit, transepts, havent, kerseymere, proceed, tide, had,\n",
      "Nearest to an: blank, beeches, circling, carriers, lanternlight, enactment, abruptly, likewise,\n",
      "Average loss at step 72000: 0.548448\n",
      "Average loss at step 74000: 0.582833\n",
      "Average loss at step 76000: 0.563111\n",
      "Average loss at step 78000: 0.574874\n",
      "Average loss at step 80000: 0.539707\n",
      "Nearest to were: are, notices, upwards, impressions, marketday, dirt, drawback, enclosures,\n",
      "Nearest to for: culminated, bump, doubts, miller, blooming, incompre, exists, earlier,\n",
      "Nearest to one: repliedi, fine, tophet, addition, milestone, transaction, party, around,\n",
      "Nearest to was: awaited, wasnt, seemed, hired, murmuring, eating, sowed, injure,\n",
      "Nearest to and: shook, kerb, hev, eastward, due, rarest, pulse, timidly,\n",
      "Nearest to his: your, yer, hurried, fannys, my, strangulation, liddys, troys,\n",
      "Nearest to as: trial, quenched, weighted, cork, demonian, affairs, newfound, vices,\n",
      "Nearest to be: repeat, follow, exhibited, take, sented, have, rural, protected,\n",
      "Nearest to so: aesthetic, dun, shudder, rude, garish, encased, loudbeating, rev,\n",
      "Nearest to at: steely, chok, jug, gallantries, minddwelt, promising, expostulated, scarn,\n",
      "Nearest to in: arrested, mination, overthrown, revenge, noteworthy, outburst, recognised, rapid,\n",
      "Nearest to now: jan, magnanimously, excitedly, ceeded, javelinmen, discerning, hnowing, jealousy,\n",
      "Nearest to the: shutting, tomor, distress, retiring, openshuttered, lowtoned, northwestbynorth, oblique,\n",
      "Nearest to said: continued, says, wiher, murmured, vital, columbus, inquired, rattled,\n",
      "Nearest to have: havent, cement, encourage, womankind, temperate, bristled, tide, ginger,\n",
      "Nearest to an: accursed, beeches, blank, circling, vigorously, stead, breeches, villagers,\n",
      "Average loss at step 82000: 0.564628\n",
      "Average loss at step 84000: 0.581268\n",
      "Average loss at step 86000: 0.641230\n",
      "Average loss at step 88000: 0.630940\n",
      "Average loss at step 90000: 0.639429\n",
      "Nearest to were: are, dirt, upwards, fooled, impressions, procedure, commence, scoffers,\n",
      "Nearest to for: conclusive, doubts, culminated, incompre, bump, venial, substance, betwixt,\n",
      "Nearest to one: milestone, tophet, party, unexplored, repliedi, moon, aid, nowadays,\n",
      "Nearest to was: is, awaited, murmuring, impressions, eating, wasnt, contrived, seemed,\n",
      "Nearest to and: unconcerned, timidly, quickening, gained, shook, wing, tiling, snares,\n",
      "Nearest to his: liddys, yer, your, hurried, fannys, dotted, longed, tearful,\n",
      "Nearest to as: quenched, newfound, complainingly, affairs, stalactite, demonian, artfulness, cork,\n",
      "Nearest to be: blindness, take, repeat, follow, heaved, sented, discern, yonder,\n",
      "Nearest to so: garish, encased, aesthetic, position, sparkling, surprised, thinly, regardful,\n",
      "Nearest to at: chok, gallantries, area, promising, journey, shrubs, expostulated, steely,\n",
      "Nearest to in: mination, hey, robbed, comedies, revenge, wake, flapped, meditated,\n",
      "Nearest to now: futurity, hnowing, weatherby, jealousy, tme, ascended, stabledoor, escapade,\n",
      "Nearest to the: oblique, witness, promontor, informant, drooped, chandeliers, pots, seats,\n",
      "Nearest to said: continued, says, sternly, thought, guides, exclaimed, approached, surveyed,\n",
      "Nearest to have: ginger, bcll, be, womankind, istics, tide, proceed, havent,\n",
      "Nearest to an: vigorously, enactment, covet, unborn, circling, blank, lanternlight, dudgeon,\n",
      "Average loss at step 92000: 0.602156\n",
      "Average loss at step 94000: 0.624458\n",
      "Average loss at step 96000: 0.618997\n",
      "Average loss at step 98000: 0.641364\n",
      "Average loss at step 100000: 0.660964\n",
      "Nearest to were: dirt, scoffers, resting, are, promises, upwards, fooled, oblivious,\n",
      "Nearest to for: buy, overtaken, shearer, miller, sedges, vernal, culminated, puddens,\n",
      "Nearest to one: around, milestone, fine, repliedi, discourse, addition, tophet, goodnight,\n",
      "Nearest to was: murmuring, is, thinks, baked, knew, friendship, wasnt, impressions,\n",
      "Nearest to and: answering, strove, oneandthirty, gained, trembled, townbred, compact, emphatic,\n",
      "Nearest to his: troys, merged, strangulation, your, yer, traneous, argued, furtively,\n",
      "Nearest to as: newfound, weighted, affairs, cork, sunrises, creeping, curate, suppose,\n",
      "Nearest to be: repeat, complements, grateful, follow, take, trail, discern, procedure,\n",
      "Nearest to so: encased, rude, miserably, dun, garish, bond, precipices, regardful,\n",
      "Nearest to at: crossed, detached, area, hearse, scarn, saturday, dislodging, absorbed,\n",
      "Nearest to in: comedies, pull, breast, concerning, robbed, horizontal, knowed, acquaintance,\n",
      "Nearest to now: futurity, jealousy, hnowing, weatherby, jan, fleed, voice, whist,\n",
      "Nearest to the: chandeliers, stared, horsehair, personality, numbers, harris, dried, hesitating,\n",
      "Nearest to said: exclaimed, says, wen, continued, inquired, thought, sail, indignant,\n",
      "Nearest to have: havent, strictly, corroborated, assure, mentions, kerseymere, poplar, be,\n",
      "Nearest to an: vigorously, enactment, bearable, beeches, accursed, re, blank, circling,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "cbow_losses = []\n",
    "\n",
    "# ConfigProto is a way of providing various configuration settings \n",
    "# required to execute the graph\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
    "    \n",
    "    # Initialize the variables in the graph\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    average_loss = 0\n",
    "    \n",
    "    # Train the Word2vec model for num_step iterations\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        # Generate a single batch of data\n",
    "        batch_data, batch_labels = create_cbow_batch(batch_size, window_size)\n",
    "        \n",
    "        # Populate the feed_dict and run the optimizer (minimize loss)\n",
    "        # and compute the loss\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        # Update the average loss variable\n",
    "        average_loss += l\n",
    "        \n",
    "        if (step+1) % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            cbow_losses.append(average_loss)\n",
    "            print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "            average_loss = 0\n",
    "            \n",
    "        # Evaluating validation set word similarities\n",
    "        if (step+1) % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            # Here we compute the top_k closest words for a given validation word\n",
    "            # in terms of the cosine distance\n",
    "            # We do this for all the words in the validation set\n",
    "            # Note: This is an expensive step\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "    cbow_final_embeddings = normalized_embeddings.eval()\n",
    "    \n",
    "\n",
    "np.save('cbow_embeddings',cbow_final_embeddings)\n",
    "\n",
    "with open('cbow_losses.csv', 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(cbow_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
